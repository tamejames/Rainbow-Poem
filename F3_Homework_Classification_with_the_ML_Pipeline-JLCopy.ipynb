{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "F3 Homework - Classification with the ML Pipeline",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tamejames/Rainbow-Poem/blob/master/F3_Homework_Classification_with_the_ML_Pipeline-JLCopy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwqvIDABWv0Y"
      },
      "source": [
        "#Training Data\n",
        "\n",
        "We will use two datasets to test your ML model and process: the 'zoo dataset' and the 'ANU coffee dataset' which we will create together. Once the 'ANU coffee dataset' has been collected, we will release for you to analyze with a similar process to the one outlined below.  \n",
        "\n",
        "[Zoo dataset](http://archive.ics.uci.edu/ml/datasets/Zoo?ref=datanews.io) -- As described in the dataset information sheet:\n",
        "\n",
        "“A simple database containing 17 Boolean-valued attributes. The \"type\" attribute appears to be the class attribute.” The datasheet will quickly reveal that the dataset itself has some problematic categories. We are primarily using it in order to have access to multiple datasets with binary-valued attributes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9mhelIBWC1l"
      },
      "source": [
        "# First, we'll download the zoo dataset to a local (temporary) folder\n",
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkWtggFhnB6q"
      },
      "source": [
        "# We can also download and display the dataset's description:\n",
        "# This command downloads the relevant file\n",
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.names\n",
        "# This command displays the file's contents\n",
        "!cat zoo.names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X2qOMfOWu7l"
      },
      "source": [
        "## Data Ingestion\n",
        "\n",
        "Here we'll \"ingest\" the data by importing it into [pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVH7vhBEXHaQ"
      },
      "source": [
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "\n",
        "# Because the data file doesn't have header names, we'll list them here\n",
        "# You can find a description of the data file at http://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.names\n",
        "feature_names = ['animal name', 'hair', 'feathers', 'eggs', 'milk', 'airborne', 'aquatic', 'predator', 'toothed', 'backbone', 'breathes', 'venomous', 'fins', 'legs', 'tail', 'domestic', 'catsize', 'type']\n",
        "\n",
        "# Import the \"zoo\" dataset\n",
        "zoo = pd.read_csv('zoo.data', names = feature_names)\n",
        "\n",
        "# Lets take a peek at the data\n",
        "zoo.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inxb_KT1XM_n"
      },
      "source": [
        "# We'll now import a few useful packages\n",
        "\n",
        "# Numpy is a linear algebra library, \n",
        "# useful for common math operations\n",
        "import numpy as np \n",
        "# Matplotlib is a common plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "# Seaborn is handy for creating beautiful plots\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqToOAvLYAQK"
      },
      "source": [
        "## Data validation\n",
        "\n",
        "Now's a good chance to have a look at your data and make sure it \"checks out\". Try plotting several aspects of the data, following the exploratory data analysis steps we've looked at in build, to check for trends and inconsistencies in the data. This is also a good opportunity to begin familiarizing yourself with Panda's capabilities, or revising Tableau or Excel. We've included a 'correlation matrix' to help you get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8aLv5-qjxlR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "f810730a-21e0-465b-a414-606265c13179"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr = zoo.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(10, 240, as_cmap=True, sep=20, n=11)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e9c142e78f44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compute the correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Generate a mask for the upper triangle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'zoo' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXyD2oPDYip2"
      },
      "source": [
        "# Feature Extraction\n",
        "\n",
        "We'll now split up the data into features and labels. We will not do any special pre-processing to generate features, but you are of course welcome to experiment with engineering new features if you have an intuition for how it will improve your model performance.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LURF_2C4Y0Du"
      },
      "source": [
        "features = zoo.loc[:, 'hair':'catsize'] # Omit animal name\n",
        "labels = zoo.loc[:, 'type']\n",
        "# We then convert the feature and labels dataframes to \n",
        "# numpy ndarrays, which can interface with the scikit-learn models\n",
        "X = features.to_numpy()\n",
        "y = labels.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIUZzdxjaLQW"
      },
      "source": [
        "# To help familiarize yourself with these matrices, \n",
        "# have a look at their 'shapes' and understand why they are so.\n",
        "print('X shape', X.shape)\n",
        "print('y shape', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzAVz5oUY8LD"
      },
      "source": [
        "# ML Algorithm and Model\n",
        "\n",
        "The model you will use to create classifications is called a “Decision Tree”. You have likely seen these charted as thought diagrams, and are a popular tool among biologists for species identification. It’s also a powerful machine learning model that’s relatively intuitive which we can use to practice classification in the ML pipeline. \n",
        "\n",
        "The decision tree model is described in the assigned pre-reading *A visual introduction to machine learning* (parts [I](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) and [II](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)) by stephanie yee and Tony Chu, 2018. \n",
        "\n",
        "We also give one example of pseudo-code for training a simple version tree (with binary splits) in: \n",
        "\n",
        "[A Course in Machine Learning, ch 1., Decision Trees](http://ciml.info/dl/v0_99/ciml-v0_99-ch01.pdf), by Hal Daumé III, 2015\n",
        "\n",
        "We encourage you to read it carefully and work with peers to understand it’s behavior. Depending on your stretch task submission, it may be to your advantage to explain how the model learns. \n",
        "\n",
        "Additional resources: To use the scikit-learn decision tree algorithm, have a look at their [documentation](https://scikit-learn.org/stable/modules/tree.html). A more advanced ensemble of decision trees is called a “random forest”, while we do not cover it in class you are welcome to learn more about this approach. You can find one useful resource visualizing decision trees and generating random forests [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sjdwb_7Y1n7"
      },
      "source": [
        "# INITIALIZE YOUR MODEL HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdOdYHS2asMD"
      },
      "source": [
        "## Quality Metric and Model Tuning\n",
        "\n",
        "First, let's begin by splitting our data into training and test sets. You can do so using scikit-learn's [`test_train_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. When setting its parameters, be mindful record your decisions. (You can also create a validation set by using `test_train_split` a second time.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuXC_2jRa9xn"
      },
      "source": [
        "# SPLIT YOUR DATA INTO TRAIN/TEST SETS HERE "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz1lHbPkbdjY"
      },
      "source": [
        "### Train your model\n",
        "\n",
        "Try training your model decision tree model below on the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFA7KnKnbwvr"
      },
      "source": [
        "# TRAIN YOUR MODEL HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI8KdAq2dkoH"
      },
      "source": [
        "## Evaluate your model\n",
        "\n",
        "Start by testing out your model's test set accuracy using the DecisionTree's `score` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BY_SdHedey6"
      },
      "source": [
        "# EVALUATE YOUR MODEL ON TEST SET HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkccRCuLc9El"
      },
      "source": [
        "## Visualizing your model\n",
        "\n",
        "A good first way to investigate your model is by visualizing it! We can do so using code similar to that provided by scikit-learn on the documentation page for the DecisionTreeClassifier model.\n",
        "\n",
        "You may also find their \"[Understanding the decision tree structure](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py)\" useful for understanding your model's behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xezV43CcdMwR"
      },
      "source": [
        "from sklearn import tree\n",
        "import graphviz\n",
        "\n",
        "# clf = # YOUR DECISON TREE CLASSIFIER HERE\n",
        "dot_data = tree.export_graphviz(clf, out_file=None, \n",
        "                      feature_names=features.columns,  \n",
        "                      class_names=sorted(list(map(str, labels.unique()))),  \n",
        "                      filled=True, rounded=True,  \n",
        "                      special_characters=True)  \n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4reWBQAeXlV"
      },
      "source": [
        "## Tuning your model\n",
        "\n",
        "Hyperparameters like max depth can drastically affect your model's performance. Use k-fold cross validation to determine a good `max_depth` for your decision tree. Plot the cross validation score for each `max_depth` setting. Be sure to record how many folds you selected.\n",
        "\n",
        "You can learn how to do k-fold cross validation with scikit-learn from the [documentation](https://scikit-learn.org/stable/modules/cross_validation.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htr7ZDEkfBkA"
      },
      "source": [
        "# YOUR CROSS VALIDATION CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8st6F_GoJlI"
      },
      "source": [
        "Next steps -- now that you've completed the ML pipeline for a classifier model, it's time for you to complete the stretch task. The stretch tasks are explained in more detail in the project handout -- a good place to start is taking some time to reflect on your skills through the skills essay, and then selecting the stretch task you think will best help you develop those skills. This will likely lead you to continuing to investigate concepts related to the code above, implementing or experimenting with a new approach, and so forth. In this sense, you can view the above as a helpful scaffold for what you will discuss in the video. We also encourage you as using the above as a framework for applying the ML pipeline to create a classifier on the ANU coffee dataset. We plan to release it by 5pm on Friday, August 27.\n",
        "\n"
      ]
    }
  ]
}